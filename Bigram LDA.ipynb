{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8aa09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "298eb42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/cep4u/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import contractions\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim \n",
    "from gensim import corpora, models\n",
    "import numpy as np\n",
    "from scipy.special import psi, polygamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "600ce979",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('News_Category_Dataset_v3.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2d6dbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>209522</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/rim-ceo-t...</td>\n",
       "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
       "      <td>TECH</td>\n",
       "      <td>Verizon Wireless and AT&amp;T are already promotin...</td>\n",
       "      <td>Reuters, Reuters</td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209523</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/maria-sha...</td>\n",
       "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Afterward, Azarenka, more effusive with the pr...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209524</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/super-bow...</td>\n",
       "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Leading up to Super Bowl XLVI, the most talked...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209525</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/aldon-smi...</td>\n",
       "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>CORRECTION: An earlier version of this story i...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209526</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/dwight-ho...</td>\n",
       "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>The five-time all-star center tore into his te...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     link  \\\n",
       "209522  https://www.huffingtonpost.com/entry/rim-ceo-t...   \n",
       "209523  https://www.huffingtonpost.com/entry/maria-sha...   \n",
       "209524  https://www.huffingtonpost.com/entry/super-bow...   \n",
       "209525  https://www.huffingtonpost.com/entry/aldon-smi...   \n",
       "209526  https://www.huffingtonpost.com/entry/dwight-ho...   \n",
       "\n",
       "                                                 headline category  \\\n",
       "209522  RIM CEO Thorsten Heins' 'Significant' Plans Fo...     TECH   \n",
       "209523  Maria Sharapova Stunned By Victoria Azarenka I...   SPORTS   \n",
       "209524  Giants Over Patriots, Jets Over Colts Among  M...   SPORTS   \n",
       "209525  Aldon Smith Arrested: 49ers Linebacker Busted ...   SPORTS   \n",
       "209526  Dwight Howard Rips Teammates After Magic Loss ...   SPORTS   \n",
       "\n",
       "                                        short_description           authors  \\\n",
       "209522  Verizon Wireless and AT&T are already promotin...  Reuters, Reuters   \n",
       "209523  Afterward, Azarenka, more effusive with the pr...                     \n",
       "209524  Leading up to Super Bowl XLVI, the most talked...                     \n",
       "209525  CORRECTION: An earlier version of this story i...                     \n",
       "209526  The five-time all-star center tore into his te...                     \n",
       "\n",
       "             date  \n",
       "209522 2012-01-28  \n",
       "209523 2012-01-28  \n",
       "209524 2012-01-28  \n",
       "209525 2012-01-28  \n",
       "209526 2012-01-28  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824c7ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    def __init__(self, df, column_name, stop_words, wnl ):\n",
    "        self.df = df\n",
    "        self.column_name = column_name\n",
    "        self.stop_words = stop_words\n",
    "        self.wnl = wnl\n",
    "    \n",
    "    def remove_null(self, df, column_name):\n",
    "        df = df[df[column_name].notnull()]\n",
    "        return df\n",
    "\n",
    "    def remove_contractions(self, df, column_name):\n",
    "        df[f'RemoveContractions_{column_name}'] = df[column_name].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
    "        return df\n",
    "\n",
    "    def rebuild_string(self, df, column_name):\n",
    "        df[f'{column_name}_string_nocont'] = [' '.join(map(str, l)) for l in df[f'RemoveContractions_{column_name}']]\n",
    "        return df\n",
    "\n",
    "    def tokenize(self, df, column_name):\n",
    "        df[f'tokenized_{column_name}'] = df[f'{column_name}_string_nocont'].apply(word_tokenize)\n",
    "        return df\n",
    "    \n",
    "    def token_cleanup(self, df, column_name):\n",
    "        edge_cases = [\"``\", \"’\", \"''\", \"image\", \"title\", \"alt\", \"src\", \"width\", \"img\", \"http\", \"cbc\", \"jpg\", \"16x9_460\", \"buzzfeed\", \"com\", \"h1\", \"href\", \"href=\", 'p', '/p', '/a' \"rel\", \"www\", \"reuters\", \"timesofindia\", \"indiatimes\", \"margin\", \"nofollow\", '8217', '8230']\n",
    "        df[f'tokenized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [word.lower() for word in x])\n",
    "        df[f'tokenized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [word for word in x if word not in string.punctuation])\n",
    "        df[f'tokenized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [word for word in x if word not in self.stop_words])\n",
    "        df[f'tokenized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [word for word in x if '/' not in word])\n",
    "        df[f'tokenized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [word for word in x if word not in edge_cases])\n",
    "        return df\n",
    "\n",
    "    def make_bigrams(self, df, column_name):\n",
    "        bigram = gensim.models.Phrases(df[f'tokenized_{column_name}'], min_count=5, threshold=100)\n",
    "        bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "        def get_bigrams(tokens_list):\n",
    "            return bigram_mod[tokens_list]\n",
    "\n",
    "        df[f'bigrams_{column_name}'] = df[f'tokenized_{column_name}'].apply(get_bigrams)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    def lemmatize_tokens(self, df, column_name):\n",
    "        clean_up = [\"'s\", \"--\"]\n",
    "        df[f'lemmatized_{column_name}'] = df[f'bigrams_{column_name}'].apply(lambda x: [self.wnl.lemmatize(word) for word in x])\n",
    "        df[f'lemmatized_{column_name}'] = df[f'lemmatized_{column_name}'].apply(lambda x: [word for word in x if word not in clean_up])\n",
    "        return df\n",
    "\n",
    "    def clean(self):\n",
    "        df = self.remove_null(self.df, self.column_name)\n",
    "        df = self.remove_contractions(df, self.column_name)\n",
    "        df = self.rebuild_string(df, self.column_name)\n",
    "        df = self.tokenize(df, self.column_name)\n",
    "        df = self.token_cleanup(df, self.column_name)\n",
    "        df = self.make_bigrams(df, self.column_name)\n",
    "        df = self.lemmatize_tokens(df, self.column_name)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d2dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "desc_cleaner_news = DataCleaner(df, 'short_description', stop_words, wnl)\n",
    "cleaned_df_desc = desc_cleaner_news.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468bbad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df_desc['lemmatized_short_description'][13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed5308",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDAModeler:\n",
    "    def __init__(self, df, column_name):\n",
    "        self.df = df\n",
    "        self.column_name = column_name\n",
    "    \n",
    "    def create_corpus(self, df, column_name):\n",
    "        \n",
    "        id2word = corpora.Dictionary(df[f'lemmatized_{self.column_name}'])\n",
    "\n",
    "        texts = df[f'lemmatized_{self.column_name}']\n",
    "\n",
    "        corpus = [id2word.doc2bow(text) for text in texts]\n",
    "        \n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3688e381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(cleaned_df_desc['lemmatized_short_description'])\n",
    "\n",
    "# Create Corpus\n",
    "texts = cleaned_df_desc['lemmatized_short_description']\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View the first entry in the corpus\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b72e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_lda_model_5 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00453f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_lda_model_5.save(\"bigram_lda_model_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2b4b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bigram_lda_model_5.print_topics())\n",
    "doc_lda = bigram_lda_model_5[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9691997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', bigram_lda_model_5.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "bigram_coherence_model_lda_5 = CoherenceModel(model=bigram_lda_model_5, texts= cleaned_df_desc['lemmatized_short_description'], dictionary=id2word, coherence='c_v')\n",
    "bigram_coherence_lda_5 = bigram_coherence_model_lda_5.get_coherence()\n",
    "print('\\nCoherence Score: ', bigram_coherence_lda_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750d5594",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "bigram_vis_5 = pyLDAvis.gensim.prepare(bigram_lda_model_5, corpus, id2word)\n",
    "bigram_vis_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de14162e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7eb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Initialization\n",
    "D = len(texts)  # Number of documents\n",
    "V = len(id2word)  # Number of unique words\n",
    "T = 10  # Number of topics\n",
    "\n",
    "# Initialize topic assignment randomly\n",
    "topic_assignment = [[random.randint(0, T-1) for _ in document] for document in texts]\n",
    "\n",
    "# Initialize Count matrices\n",
    "# N_{d, t}: count of words in document d assigned to topic t\n",
    "ndt = np.zeros((D, T))  \n",
    "# N_{t, v}: count of assignments to topic t of word v\n",
    "ntv = np.zeros((T, V))\n",
    "# N_{t}: total count of words assigned to topic t\n",
    "nt = np.zeros(T)\n",
    "\n",
    "# Iterate over corpus to fill Count matrices\n",
    "for d in range(D):\n",
    "    for i, v in enumerate(texts[d]):\n",
    "        t = topic_assignment[d][i]\n",
    "        ntv[t, id2word.token2id[v]] += 1\n",
    "        ndt[d, t] += 1\n",
    "        nt[t] += 1\n",
    "\n",
    "# Iteratively update topic assignments\n",
    "for d in range(D):\n",
    "    for i, v in enumerate(texts[d]):\n",
    "        t = topic_assignment[d][i]\n",
    "        \n",
    "        # Decrement count matrices for old assignment\n",
    "        ntv[t, id2word.token2id[v]] -= 1\n",
    "        ndt[d, t] -= 1\n",
    "        nt[t] -= 1\n",
    "        \n",
    "        # Compute conditional distribution for new assignment\n",
    "        p = ((ndt[d, :] + 0.001) / (ndt[d, :].sum() + 0.001 * T)) * ((ntv[:, id2word.token2id[v]] + 0.001) / (nt.sum() + 0.001 * V))\n",
    "        assert np.all(p >= 0), \"Negative probabilities found!\"\n",
    "        assert np.all(p <= 1), \"Probabilities above 1 found!\"\n",
    "        assert not np.isnan(p).any(), \"Probabilities are NaN!\"\n",
    "        t = np.random.multinomial(1, p / p.sum()).argmax()\n",
    "        \n",
    "        # Increment count matrices for new assignment\n",
    "        ntv[t, id2word.token2id[v]] += 1\n",
    "        ndt[d, t] += 1\n",
    "        nt[t] += 1\n",
    "\n",
    "        # Update topic assignment\n",
    "        topic_assignment[d][i] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be34bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10  # Number of words to display per topic\n",
    "\n",
    "for t in range(T):\n",
    "    print(f\"Topic {t}:\")\n",
    "    \n",
    "    # Get the top N word indices for this topic\n",
    "    top_word_indices = ntv[t].argsort()[::-1][:N]\n",
    "    \n",
    "    # Print the words\n",
    "    for i in top_word_indices:\n",
    "        print(f\"\\t{id2word[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfc3b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialization\n",
    "N = sum(len(doc) for doc in texts)  # Total number of words in all documents\n",
    "D = len(texts)  # Number of documents\n",
    "V = len(id2word)  # Number of unique words\n",
    "T = 10  # Number of topics\n",
    "alpha = 0.1  # Prior on the topic distributions per document\n",
    "\n",
    "# Initialize phi and gamma\n",
    "phi = np.ones((N, T)) / T\n",
    "gamma = np.full((T,), alpha + N / T)\n",
    "\n",
    "# Initialize term-topic matrix beta (you might use different method to initialize it)\n",
    "beta = np.random.dirichlet(alpha=np.ones(len(id2word)), size=T)\n",
    "\n",
    "# Map from word to its index\n",
    "word2id = {word: i for doc in texts for word in doc}\n",
    "\n",
    "\n",
    "# Expectation step\n",
    "def e_step():\n",
    "    global gamma  # Ensure we're using the global gamma variable\n",
    "    for n, doc in enumerate(texts):\n",
    "        for i, word in enumerate(doc):\n",
    "            for t in range(T):\n",
    "                phi[n][t] = beta[t][id2word.token2id[word]] * np.exp(psi(gamma[t]))\n",
    "            # Normalize phi\n",
    "            phi[n] /= phi[n].sum()\n",
    "        # Update gamma after processing each document\n",
    "        gamma = alpha + phi.sum(axis=0)\n",
    "\n",
    "# Maximization step\n",
    "def m_step():\n",
    "    for t in range(T):\n",
    "        for n, doc in enumerate(texts):\n",
    "            for i, word in enumerate(doc):\n",
    "                beta[t][word2id[word]] += phi[n][t]\n",
    "        # Normalize beta\n",
    "        beta[t] /= beta[t].sum()\n",
    "\n",
    "# Iterate until convergence\n",
    "max_iter = 100\n",
    "for iteration in range(max_iter):\n",
    "    e_step()\n",
    "    m_step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c4ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(beta, id2word, n_top_words):\n",
    "    for i, topic_dist in enumerate(beta):\n",
    "        topic_words = np.array(id2word)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "        print('Topic {}: {}'.format(i, ' '.join(topic_words)))\n",
    "\n",
    "# Call the function with your beta and id2word dictionary:\n",
    "n_top_words = 10\n",
    "display_topics(beta, list(id2word.values()), n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc53017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity\n",
    "def compute_perplexity():\n",
    "    log_likelihood = 0\n",
    "    for n, doc in enumerate(texts):\n",
    "        for i, word in enumerate(doc):\n",
    "            log_likelihood += np.log(np.sum(phi[n, :] * beta[:, word2id[word]]))\n",
    "    perplexity = np.exp(-1. * log_likelihood / N)\n",
    "    return perplexity\n",
    "\n",
    "print(\"Perplexity: \", compute_perplexity())\n",
    "\n",
    "# Coherence (UMass measure)\n",
    "def compute_coherence():\n",
    "    topic_words = beta.argsort(axis=-1)[:, :10]  # Top 10 words per topic\n",
    "    total_score = 0\n",
    "    for t in range(T):\n",
    "        for i in range(len(topic_words[t]) - 1):\n",
    "            for j in range(i + 1, len(topic_words[t])):\n",
    "                score = np.log((beta[t, topic_words[t, j]] + 1.) / beta[t, topic_words[t, i]])\n",
    "                total_score += score\n",
    "    coherence = total_score / T\n",
    "    return coherence\n",
    "\n",
    "print(\"Coherence: \", compute_coherence())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43077c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Initialization\n",
    "D = len(texts)  # Number of documents\n",
    "V = len(id2word)  # Number of unique words\n",
    "T = 10  # Number of topics\n",
    "alpha = 0.1  # Prior on the topic distributions per document\n",
    "beta = 0.01  # Prior on the word distributions per topic\n",
    "\n",
    "# Initialize topic assignment randomly\n",
    "topic_assignment = [[random.randint(0, T-1) for _ in document] for document in texts]\n",
    "\n",
    "# Initialize Count matrices\n",
    "# N_{d, t}: count of words in document d assigned to topic t\n",
    "ndt = np.zeros((D, T))  \n",
    "# N_{t, v}: count of assignments to topic t of word v\n",
    "ntv = np.zeros((T, V))\n",
    "# N_{t}: total count of words assigned to topic t\n",
    "nt = np.zeros(T)\n",
    "\n",
    "# Iterate over corpus to fill Count matrices\n",
    "for d in range(D):\n",
    "    for i, v in enumerate(texts[d]):\n",
    "        t = topic_assignment[d][i]\n",
    "        ntv[t, id2word.token2id[v]] += 1\n",
    "        ndt[d, t] += 1\n",
    "        nt[t] += 1\n",
    "\n",
    "# Collapsed Gibbs Sampling\n",
    "for iteration in range(100):  # Choose the number of iterations\n",
    "    for d in range(D):\n",
    "        for i, v in enumerate(texts[d]):\n",
    "            t = topic_assignment[d][i]\n",
    "            \n",
    "            # Decrement count matrices for old assignment\n",
    "            ntv[t, id2word.token2id[v]] -= 1\n",
    "            ndt[d, t] -= 1\n",
    "            nt[t] -= 1\n",
    "            \n",
    "            # Sample new topic assignment from conditional distribution\n",
    "            p = ((ndt[d, :] + alpha) / (np.sum(ndt[d, :]) + T * alpha)) * ((ntv[:, id2word.token2id[v]] + beta) / (nt + V * beta))\n",
    "            t = np.random.multinomial(1, p / p.sum()).argmax()\n",
    "            \n",
    "            # Increment count matrices for new assignment\n",
    "            ntv[t, id2word.token2id[v]] += 1\n",
    "            ndt[d, t] += 1\n",
    "            nt[t] += 1\n",
    "            \n",
    "            # Update topic assignment\n",
    "            topic_assignment[d][i] = t\n",
    "\n",
    "# Get the word distributions for each topic\n",
    "phi = (ntv + beta) / (nt[:, None] + V * beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f892a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_lda_model_10 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "bigram_lda_model_10.save(\"bigram_lda_model_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bfc886",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bigram_lda_model_10.print_topics())\n",
    "doc_lda = bigram_lda_model_10[corpus]\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', bigram_lda_model_10.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda_10 = CoherenceModel(model=bigram_lda_model_10, texts= cleaned_df_desc['lemmatized_short_description'], dictionary=id2word, coherence='c_v')\n",
    "coherence_lda_10 = coherence_model_lda_10.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5ac153",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "bigram_vis_10 = pyLDAvis.gensim.prepare(bigram_lda_model_10, corpus, id2word)\n",
    "bigram_vis_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb4b072",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    def __init__(self, df, column_name, stop_words, wnl ):\n",
    "        self.df = df\n",
    "        self.column_name = column_name\n",
    "        self.stop_words = stop_words\n",
    "        self.wnl = wnl\n",
    "    \n",
    "    def remove_null(self, df, column_name):\n",
    "        df = df[df[column_name].notnull()]\n",
    "        return df\n",
    "\n",
    "    def remove_contractions(self, df, column_name):\n",
    "        df[f'RemoveContractions_{column_name}'] = df[column_name].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
    "        return df\n",
    "\n",
    "    def rebuild_string(self, df, column_name):\n",
    "        df[f'{column_name}_string_nocont'] = [' '.join(map(str, l)) for l in df[f'RemoveContractions_{column_name}']]\n",
    "        return df\n",
    "\n",
    "    def tokenize(self, df, column_name):\n",
    "        df[f'tokenized_{column_name}'] = df[f'{column_name}_string_nocont'].apply(word_tokenize)\n",
    "        return df\n",
    "    \n",
    "    def token_cleanup(self, df, column_name):\n",
    "        edge_cases = [\"``\", \"’\", \"''\", \"image\", \"title\", \"alt\", \"src\", \"width\", \"img\", \"http\", \"cbc\", \"jpg\", \"16x9_460\", \"buzzfeed\", \"com\", \"h1\", \"href\", \"href=\", 'p', '/p', '/a' \"rel\", \"www\", \"reuters\", \"timesofindia\", \"indiatimes\", \"margin\", \"nofollow\", '8217', '8230']\n",
    "        df[f'tokenized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [word.lower() for word in x])\n",
    "        df[f'tokenized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [word for word in x if word not in string.punctuation])\n",
    "        df[f'tokenized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [word for word in x if word not in self.stop_words])\n",
    "        df[f'tokenized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [word for word in x if '/' not in word])\n",
    "        df[f'tokenized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [word for word in x if word not in edge_cases])\n",
    "        return df\n",
    "\n",
    "    def make_bigrams(self, df, column_name):\n",
    "        bigram = gensim.models.Phrases(df[f'tokenized_{column_name}'], min_count=5, threshold=100)\n",
    "        bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "        def get_bigrams(tokens_list):\n",
    "            return bigram_mod[tokens_list]\n",
    "\n",
    "        df[f'bigrams_{column_name}'] = df[f'tokenized_{column_name}'].apply(get_bigrams)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    def lemmatize_tokens(self, df, column_name):\n",
    "        clean_up = [\"'s\", \"--\"]\n",
    "        df[f'lemmatized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [self.wnl.lemmatize(word) for word in x])\n",
    "        df[f'lemmatized_{column_name}'] = df[f'lemmatized_{column_name}'].apply(lambda x: [word for word in x if word not in clean_up])\n",
    "        return df\n",
    "\n",
    "\n",
    "    def clean(self):\n",
    "        df = self.remove_null(self.df, self.column_name)\n",
    "        df = self.remove_contractions(df, self.column_name)\n",
    "        df = self.rebuild_string(df, self.column_name)\n",
    "        df = self.tokenize(df, self.column_name)\n",
    "        df = self.token_cleanup(df, self.column_name)\n",
    "       # df = self.make_bigrams(df, self.column_name)\n",
    "        df = self.lemmatize_tokens(df, self.column_name)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3e87b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('News_Category_Dataset_v3.json', lines=True)\n",
    "wnl = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "desc_cleaner_news_uni = DataCleaner(df, 'short_description', stop_words, wnl)\n",
    "cleaned_df_desc_uni = desc_cleaner_news_uni.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea59c756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word_uni = corpora.Dictionary(cleaned_df_desc_uni['lemmatized_short_description'])\n",
    "\n",
    "# Create Corpus\n",
    "texts_uni = cleaned_df_desc_uni['lemmatized_short_description']\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus_uni = [id2word_uni.doc2bow(text) for text in texts_uni]\n",
    "\n",
    "# View the first entry in the corpus\n",
    "print(corpus_uni[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f2c252",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_uni = gensim.models.ldamodel.LdaModel(corpus=corpus_uni,\n",
    "                                           id2word=id2word_uni,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9473a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_uni.save(\"unigram_lda_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db5317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_model_uni.print_topics())\n",
    "doc_lda_uni = lda_model_uni[corpus_uni]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671ff682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model_uni.log_perplexity(corpus_uni))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda_uni = CoherenceModel(model=lda_model_uni, texts= cleaned_df_desc['lemmatized_short_description'], dictionary=id2word_uni, coherence='c_v')\n",
    "coherence_lda_uni = coherence_model_lda_uni.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e719a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model_uni, corpus_uni, id2word_uni)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aaf685",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb01dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.save(\"bigram_lda_model_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fa570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8892964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts= cleaned_df_desc['lemmatized_short_description'], dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4e167a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probstoch_conda",
   "language": "python",
   "name": "probstoch_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
