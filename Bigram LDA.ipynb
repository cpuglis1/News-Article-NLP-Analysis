{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8aa09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "298eb42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/cep4u/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import contractions\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim \n",
    "from gensim import corpora, models\n",
    "import numpy as np\n",
    "from scipy.special import psi, polygamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "600ce979",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('News_Category_Dataset_v3.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9f371fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>209522</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/rim-ceo-t...</td>\n",
       "      <td>RIM CEO Thorsten Heins' 'Significant' Plans Fo...</td>\n",
       "      <td>TECH</td>\n",
       "      <td>Verizon Wireless and AT&amp;T are already promotin...</td>\n",
       "      <td>Reuters, Reuters</td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209523</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/maria-sha...</td>\n",
       "      <td>Maria Sharapova Stunned By Victoria Azarenka I...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Afterward, Azarenka, more effusive with the pr...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209524</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/super-bow...</td>\n",
       "      <td>Giants Over Patriots, Jets Over Colts Among  M...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>Leading up to Super Bowl XLVI, the most talked...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209525</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/aldon-smi...</td>\n",
       "      <td>Aldon Smith Arrested: 49ers Linebacker Busted ...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>CORRECTION: An earlier version of this story i...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209526</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/dwight-ho...</td>\n",
       "      <td>Dwight Howard Rips Teammates After Magic Loss ...</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>The five-time all-star center tore into his te...</td>\n",
       "      <td></td>\n",
       "      <td>2012-01-28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     link  \\\n",
       "209522  https://www.huffingtonpost.com/entry/rim-ceo-t...   \n",
       "209523  https://www.huffingtonpost.com/entry/maria-sha...   \n",
       "209524  https://www.huffingtonpost.com/entry/super-bow...   \n",
       "209525  https://www.huffingtonpost.com/entry/aldon-smi...   \n",
       "209526  https://www.huffingtonpost.com/entry/dwight-ho...   \n",
       "\n",
       "                                                 headline category  \\\n",
       "209522  RIM CEO Thorsten Heins' 'Significant' Plans Fo...     TECH   \n",
       "209523  Maria Sharapova Stunned By Victoria Azarenka I...   SPORTS   \n",
       "209524  Giants Over Patriots, Jets Over Colts Among  M...   SPORTS   \n",
       "209525  Aldon Smith Arrested: 49ers Linebacker Busted ...   SPORTS   \n",
       "209526  Dwight Howard Rips Teammates After Magic Loss ...   SPORTS   \n",
       "\n",
       "                                        short_description           authors  \\\n",
       "209522  Verizon Wireless and AT&T are already promotin...  Reuters, Reuters   \n",
       "209523  Afterward, Azarenka, more effusive with the pr...                     \n",
       "209524  Leading up to Super Bowl XLVI, the most talked...                     \n",
       "209525  CORRECTION: An earlier version of this story i...                     \n",
       "209526  The five-time all-star center tore into his te...                     \n",
       "\n",
       "             date  \n",
       "209522 2012-01-28  \n",
       "209523 2012-01-28  \n",
       "209524 2012-01-28  \n",
       "209525 2012-01-28  \n",
       "209526 2012-01-28  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "824c7ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    def __init__(self, df, column_name, stop_words, wnl ):\n",
    "        self.df = df\n",
    "        self.column_name = column_name\n",
    "        self.stop_words = stop_words\n",
    "        self.wnl = wnl\n",
    "    \n",
    "    def remove_null(self, df, column_name):\n",
    "        df = df[df[column_name].notnull()]\n",
    "        return df\n",
    "\n",
    "    def remove_contractions(self, df, column_name):\n",
    "        df[f'RemoveContractions_{column_name}'] = df[column_name].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
    "        return df\n",
    "\n",
    "    def rebuild_string(self, df, column_name):\n",
    "        df[f'{column_name}_string_nocont'] = [' '.join(map(str, l)) for l in df[f'RemoveContractions_{column_name}']]\n",
    "        return df\n",
    "\n",
    "    def tokenize(self, df, column_name):\n",
    "        df[f'tokenized_{column_name}'] = df[f'{column_name}_string_nocont'].apply(word_tokenize)\n",
    "        return df\n",
    "    \n",
    "    def token_cleanup(self, df, column_name):\n",
    "        edge_cases = [\"``\", \"â€™\", \"''\", \"image\", \"title\", \"alt\", \"src\", \"width\", \"img\", \"http\", \"cbc\", \"jpg\", \"16x9_460\", \"buzzfeed\", \"com\", \"h1\", \"href\", \"href=\", 'p', '/p', '/a' \"rel\", \"www\", \"reuters\", \"timesofindia\", \"indiatimes\", \"margin\", \"nofollow\", '8217', '8230']\n",
    "        df[f'tokenized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [word.lower() for word in x])\n",
    "        df[f'tokenized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [word for word in x if word not in string.punctuation])\n",
    "        df[f'tokenized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [word for word in x if word not in self.stop_words])\n",
    "        df[f'tokenized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [word for word in x if '/' not in word])\n",
    "        df[f'tokenized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [word for word in x if word not in edge_cases])\n",
    "        return df\n",
    "\n",
    "    def make_bigrams(self, df, column_name):\n",
    "        bigram = gensim.models.Phrases(df[f'tokenized_{column_name}'], min_count=5, threshold=100)\n",
    "        bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "        def get_bigrams(tokens_list):\n",
    "            return bigram_mod[tokens_list]\n",
    "\n",
    "        df[f'bigrams_{column_name}'] = df[f'tokenized_{column_name}'].apply(get_bigrams)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    def lemmatize_tokens(self, df, column_name):\n",
    "        clean_up = [\"'s\", \"--\"]\n",
    "        df[f'lemmatized_{column_name}'] = df[f'bigrams_{column_name}'].apply(lambda x: [self.wnl.lemmatize(word) for word in x])\n",
    "        df[f'lemmatized_{column_name}'] = df[f'lemmatized_{column_name}'].apply(lambda x: [word for word in x if word not in clean_up])\n",
    "        return df\n",
    "\n",
    "    def clean(self):\n",
    "        df = self.remove_null(self.df, self.column_name)\n",
    "        df = self.remove_contractions(df, self.column_name)\n",
    "        df = self.rebuild_string(df, self.column_name)\n",
    "        df = self.tokenize(df, self.column_name)\n",
    "        df = self.token_cleanup(df, self.column_name)\n",
    "        df = self.make_bigrams(df, self.column_name)\n",
    "        df = self.lemmatize_tokens(df, self.column_name)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8d2dd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "desc_cleaner_news = DataCleaner(df, 'short_description', stop_words, wnl)\n",
    "cleaned_df_desc = desc_cleaner_news.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "468bbad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " 'man',\n",
       " 'claim',\n",
       " 'scammed',\n",
       " 'people',\n",
       " 'platform',\n",
       " 'caused',\n",
       " 'several',\n",
       " 'popular',\n",
       " 'streamer',\n",
       " 'consider',\n",
       " 'twitch',\n",
       " 'boycott']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df_desc['lemmatized_short_description'][13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91ed5308",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDAModeler:\n",
    "    def __init__(self, df, column_name):\n",
    "        self.df = df\n",
    "        self.column_name = column_name\n",
    "    \n",
    "    def create_corpus(self, df, column_name):\n",
    "        \n",
    "        id2word = corpora.Dictionary(df[f'lemmatized_{self.column_name}'])\n",
    "\n",
    "        texts = df[f'lemmatized_{self.column_name}']\n",
    "\n",
    "        corpus = [id2word.doc2bow(text) for text in texts]\n",
    "        \n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3688e381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(cleaned_df_desc['lemmatized_short_description'])\n",
    "\n",
    "# Create Corpus\n",
    "texts = cleaned_df_desc['lemmatized_short_description']\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View the first entry in the corpus\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80b72e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_lda_model_5 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00453f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_lda_model_5.save(\"bigram_lda_model_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c2b4b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.029*\"one\" + 0.017*\"u\" + 0.016*\"people\" + 0.015*\"week\" + 0.012*\"take\" + 0.011*\"thing\" + 0.008*\"much\" + 0.008*\"good\" + 0.008*\"go\" + 0.008*\"something\"'), (1, '0.019*\"day\" + 0.018*\"like\" + 0.014*\"love\" + 0.011*\"fashion\" + 0.010*\"child\" + 0.009*\"two\" + 0.009*\"woman\" + 0.008*\"home\" + 0.007*\"best\" + 0.007*\"photo\"'), (2, '0.025*\"year\" + 0.022*\"make\" + 0.018*\"know\" + 0.015*\"want\" + 0.015*\"need\" + 0.013*\"think\" + 0.012*\"come\" + 0.012*\"say\" + 0.012*\"last\" + 0.010*\"today\"'), (3, '0.030*\"look\" + 0.023*\"many\" + 0.017*\"even\" + 0.016*\"show\" + 0.014*\"every\" + 0.012*\"work\" + 0.011*\"check\" + 0.011*\"wedding\" + 0.010*\"new_york\" + 0.009*\"might\"'), (4, '0.030*\"time\" + 0.022*\"new\" + 0.020*\"life\" + 0.020*\"get\" + 0.019*\"way\" + 0.018*\"would\" + 0.018*\"may\" + 0.013*\"back\" + 0.013*\"see\" + 0.013*\"world\"')]\n"
     ]
    }
   ],
   "source": [
    "print(bigram_lda_model_5.print_topics())\n",
    "doc_lda = bigram_lda_model_5[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9691997d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -9.584815517078322\n",
      "\n",
      "Coherence Score:  0.40842829804234393\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', bigram_lda_model_5.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "bigram_coherence_model_lda_5 = CoherenceModel(model=bigram_lda_model_5, texts= cleaned_df_desc['lemmatized_short_description'], dictionary=id2word, coherence='c_v')\n",
    "bigram_coherence_lda_5 = bigram_coherence_model_lda_5.get_coherence()\n",
    "print('\\nCoherence Score: ', bigram_coherence_lda_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "750d5594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el20341122329850569314313592\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el20341122329850569314313592_data = {\"mdsDat\": {\"x\": [0.3401720440096633, 0.0318366693907697, -0.11628281871824543, -0.22067542020085892, -0.03505047448132882], \"y\": [-0.15189400846703563, 0.355130072328407, -0.023638344213321824, -0.16872849672128631, -0.010869222926763355], \"topics\": [1, 2, 3, 4, 5], \"cluster\": [1, 1, 1, 1, 1], \"Freq\": [25.42514231687737, 24.95555775284086, 18.507723760896422, 17.87900944757985, 13.232566721805503]}, \"tinfo\": {\"Term\": [\"one\", \"time\", \"look\", \"year\", \"new\", \"make\", \"day\", \"many\", \"like\", \"life\", \"get\", \"u\", \"way\", \"would\", \"may\", \"people\", \"know\", \"week\", \"love\", \"want\", \"need\", \"even\", \"take\", \"show\", \"back\", \"see\", \"world\", \"first\", \"think\", \"fashion\", \"day\", \"like\", \"love\", \"fashion\", \"child\", \"two\", \"woman\", \"home\", \"best\", \"photo\", \"find\", \"right\", \"help\", \"still\", \"night\", \"parent\", \"style\", \"often\", \"made\", \"divorce\", \"couple\", \"got\", \"mother\", \"video\", \"give\", \"ever\", \"found\", \"three\", \"favorite\", \"black\", \"one\", \"u\", \"people\", \"week\", \"take\", \"thing\", \"much\", \"good\", \"go\", \"something\", \"really\", \"said\", \"great\", \"health\", \"never\", \"let\", \"going\", \"place\", \"heart\", \"another\", \"getting\", \"looking\", \"live\", \"...\", \"friend\", \"relationship\", \"experience\", \"important\", \"person\", \"yet\", \"time\", \"new\", \"life\", \"get\", \"way\", \"would\", \"may\", \"back\", \"see\", \"world\", \"first\", \"also\", \"could\", \"little\", \"family\", \"around\", \"well\", \"american\", \"always\", \"better\", \"big\", \"part\", \"long\", \"change\", \"star\", \"making\", \"next\", \"become\", \"living\", \"recently\", \"year\", \"make\", \"know\", \"want\", \"need\", \"think\", \"come\", \"say\", \"last\", \"today\", \"food\", \"kid\", \"feel\", \"2012\", \"since\", \"according\", \"month\", \"whether\", \"city\", \"keep\", \"news\", \"mom\", \"report\", \"different\", \"use\", \"everyone\", \"problem\", \"though\", \"call\", \"turn\", \"look\", \"many\", \"even\", \"show\", \"every\", \"work\", \"check\", \"wedding\", \"new_york\", \"might\", \"marriage\", \"mean\", \"read\", \"dress\", \"study\", \"lot\", \"moment\", \"without\", \"question\", \"spring\", \"issue\", \"word\", \"together\", \"must\", \"away\", \"high\", \"february\", \"case\", \"party\", \"choice\"], \"Freq\": [15807.0, 11992.0, 8709.0, 9596.0, 8854.0, 8449.0, 10590.0, 6742.0, 9924.0, 8008.0, 7863.0, 9359.0, 7697.0, 7370.0, 7341.0, 8592.0, 6895.0, 8403.0, 7560.0, 5934.0, 5737.0, 4841.0, 6716.0, 4586.0, 5407.0, 5266.0, 5243.0, 5235.0, 5097.0, 6298.0, 10589.880100802171, 9924.168480101329, 7559.968303352073, 6297.769679638734, 5648.254824650041, 4977.338721628434, 4831.300153445276, 4628.627059530189, 4151.561289862756, 3963.9882089440225, 3550.0216943365954, 3503.2533012530735, 3470.3398889270456, 3443.659374118256, 3115.9381613046867, 2987.174802251846, 2996.374063862621, 2902.6861615207695, 2853.8305085073953, 2696.2374841323735, 2599.0055722506218, 2492.6809021372683, 2398.431084872369, 2389.020879522699, 2362.0654673720237, 2352.6885282110056, 2286.117469291894, 2128.386867307214, 2063.2118278630533, 2034.7495089419192, 15807.294291223323, 9358.96152240261, 8591.997912183135, 8402.581614261062, 6716.311793696832, 5749.772877329901, 4520.94140904857, 4518.084757493067, 4116.588532052235, 4087.895965418463, 3909.416243821944, 3558.9172980861686, 3501.059093601432, 3406.222109300882, 3212.7182634459887, 2932.258409972466, 2741.983294158508, 2688.270853968259, 2644.612379437779, 2501.8410553996678, 2495.207174430486, 2447.563410272024, 2409.512690057704, 2350.1211172296, 2301.691685303259, 2310.5377880157603, 2283.5253962026004, 2108.7028011344964, 2063.8530426640928, 2050.738161953757, 11991.636018840107, 8853.565135617302, 8008.324810944931, 7862.766264512767, 7696.543861787654, 7369.476014001448, 7340.982117976564, 5406.484001991615, 5265.342429928754, 5243.302610094497, 5235.024678273945, 4786.962893743938, 4501.669148898503, 3609.178273760921, 3553.948261056423, 3292.957824228206, 3239.6883249241755, 3132.5366189492893, 3036.8459571783087, 2935.202047419732, 2699.0752530113, 2693.881506863832, 2538.839887282983, 2409.701860738406, 2102.066635293245, 2024.9424709274845, 1906.8763518308606, 1711.749328902367, 1678.4915580137258, 1635.4834053702853, 9596.228800921437, 8449.192211352874, 6894.677589263665, 5933.596373611171, 5736.638381964596, 5096.754394769983, 4852.019139580192, 4848.369855221014, 4742.539520765414, 4020.6119178570993, 3361.995517906424, 3273.515786088771, 3197.707724537993, 3008.162727093307, 2834.4873292374036, 2656.6842723383884, 2636.701883579155, 2500.6128089754216, 2294.7114956221335, 2084.270156269225, 2030.4297928179835, 1962.2523333617103, 1958.4052071601625, 1921.5538545653053, 1810.1843112031534, 1809.2540374622183, 1802.5683976414002, 1782.9873141041337, 1779.8735265007033, 1765.3719551968354, 8709.302493465811, 6741.8878844681, 4840.444787044993, 4586.2548124760515, 4095.89347268939, 3488.508744847587, 3274.8123918063156, 3154.740493518305, 2788.7074777273774, 2534.21257160745, 2517.5003899486974, 2419.65333112101, 2283.2383364582297, 2257.0410656396043, 2153.857302031183, 2106.184770410796, 1980.4257376415133, 1980.4592908491481, 1949.5894003352971, 1890.057017731475, 1705.9813021773953, 1678.238839040771, 1637.3333183361751, 1569.5361096277434, 1396.9927657505827, 1367.8471074706702, 1412.935639464684, 1359.407670473529, 1331.7162082125096, 1315.0151662186918], \"Total\": [15807.0, 11992.0, 8709.0, 9596.0, 8854.0, 8449.0, 10590.0, 6742.0, 9924.0, 8008.0, 7863.0, 9359.0, 7697.0, 7370.0, 7341.0, 8592.0, 6895.0, 8403.0, 7560.0, 5934.0, 5737.0, 4841.0, 6716.0, 4586.0, 5407.0, 5266.0, 5243.0, 5235.0, 5097.0, 6298.0, 10590.546735437592, 9924.833912616785, 7560.639138887097, 6298.445244746431, 5648.919415547784, 4978.007441819114, 4831.966287072309, 4629.293842868198, 4152.227184217381, 3964.6568132645284, 3550.6864618782415, 3503.918843779455, 3471.0040548924494, 3444.3258558780585, 3116.608100878848, 2987.83886118315, 2997.04215963752, 2903.3522410230653, 2854.4969866143897, 2696.9035912227714, 2599.6745811341148, 2493.3471857487757, 2399.0944809115153, 2389.6893831430016, 2362.7324547308954, 2353.353298696172, 2286.783999512306, 2129.0547437621635, 2063.87858713408, 2035.4193517719207, 15807.947669009445, 9359.61367945955, 8592.650652145714, 8403.236711926618, 6716.965275738983, 5750.423942010285, 4521.593286759565, 4518.736643359846, 4117.241910073726, 4088.548342354309, 3910.07097475216, 3559.5699954829183, 3501.713189325553, 3406.8761939236215, 3213.370445846175, 2932.9102470958373, 2742.636238925677, 2688.925336717472, 2645.2739851359233, 2502.492747335469, 2495.860604844866, 2448.2199092217643, 2410.167992576277, 2350.7747497189016, 2302.342271100124, 2311.191234045703, 2284.1782001228985, 2109.3552595212564, 2064.50569883506, 2051.391653728723, 11992.295252534948, 8854.225762029379, 8008.9834890966995, 7863.425574534276, 7697.203539356001, 7370.134389064714, 7341.642999223066, 5407.146147954795, 5266.000599637058, 5243.961897644012, 5235.684805761017, 4787.623167227514, 4502.329114368576, 3609.837514814134, 3554.60603127127, 3293.6178019672984, 3240.349908842749, 3133.1978751451743, 3037.505894493547, 2935.860987865105, 2699.735861241278, 2694.541208419145, 2539.4988168872287, 2410.360448376022, 2102.729374866158, 2025.602441579075, 1907.5354282519208, 1712.407951724466, 1679.1502111414898, 1636.142649492346, 9596.88853808691, 8449.850954072517, 6895.335237109135, 5934.251522676707, 5737.2970101474, 5097.413026994598, 4852.67734267886, 4849.029016178472, 4743.199061620772, 4021.2756068549243, 3362.653511997062, 3274.172452782358, 3198.3638863334268, 3008.827439694105, 2835.1482475769035, 2657.3444273912382, 2637.3600346936355, 2501.27151816822, 2295.368355342672, 2084.9287029373872, 2031.0878138330627, 1962.9085589412662, 1959.066520886773, 1922.211707751872, 1810.8423027676301, 1809.9126422426896, 1803.2269226611118, 1783.6457896585903, 1780.533122079306, 1766.030750359052, 8709.972814532419, 6742.555950651608, 4841.112131597235, 4586.92804826218, 4096.561261380057, 3489.175656034523, 3275.4797322185227, 3155.4056794852136, 2789.3791503519406, 2534.880289422315, 2518.172771739284, 2420.3205163078906, 2283.910276907907, 2257.708955478721, 2154.5251016718357, 2106.8508577028406, 1981.0922432768482, 1981.1274729476625, 1950.2586679207006, 1890.7288574665174, 1706.6527916115353, 1678.909461779911, 1638.0031634726877, 1570.2090490710877, 1397.6583714417807, 1368.5175836368076, 1413.629488058869, 1360.0769285483443, 1332.3860201539726, 1315.6845320089258], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -3.9569, -4.0218, -4.2939, -4.4766, -4.5854, -4.7119, -4.7417, -4.7845, -4.8933, -4.9395, -5.0498, -5.0631, -5.0725, -5.0802, -5.1803, -5.2225, -5.2194, -5.2511, -5.2681, -5.3249, -5.3617, -5.4034, -5.442, -5.4459, -5.4572, -5.4612, -5.4899, -5.5614, -5.5925, -5.6064, -3.5377, -4.0618, -4.1473, -4.1696, -4.3936, -4.549, -4.7894, -4.7901, -4.8831, -4.8901, -4.9348, -5.0287, -5.0451, -5.0725, -5.131, -5.2224, -5.2895, -5.3092, -5.3256, -5.3811, -5.3838, -5.403, -5.4187, -5.4437, -5.4645, -5.4607, -5.4724, -5.5521, -5.5736, -5.5799, -3.515, -3.8184, -3.9187, -3.9371, -3.9585, -4.0019, -4.0058, -4.3116, -4.3381, -4.3423, -4.3439, -4.4333, -4.4948, -4.7158, -4.7312, -4.8074, -4.8238, -4.8574, -4.8884, -4.9225, -5.0063, -5.0082, -5.0675, -5.1197, -5.2563, -5.2937, -5.3538, -5.4617, -5.4813, -5.5073, -3.7033, -3.8306, -4.0339, -4.184, -4.2178, -4.3361, -4.3853, -4.386, -4.4081, -4.5732, -4.7521, -4.7788, -4.8022, -4.8633, -4.9228, -4.9876, -4.9951, -5.0481, -5.1341, -5.2303, -5.2564, -5.2906, -5.2925, -5.3115, -5.3712, -5.3718, -5.3755, -5.3864, -5.3881, -5.3963, -3.4993, -3.7554, -4.0867, -4.1407, -4.2537, -4.4142, -4.4775, -4.5148, -4.6381, -4.7338, -4.7405, -4.7801, -4.8381, -4.8497, -4.8965, -4.9188, -4.9804, -4.9804, -4.9961, -5.0271, -5.1296, -5.146, -5.1707, -5.2129, -5.3294, -5.3505, -5.3181, -5.3567, -5.3773, -5.3899], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 1.3694, 1.3694, 1.3693, 1.3693, 1.3693, 1.3693, 1.3693, 1.3693, 1.3693, 1.3693, 1.3692, 1.3692, 1.3692, 1.3692, 1.3692, 1.3692, 1.3692, 1.3692, 1.3692, 1.3692, 1.3692, 1.3692, 1.3692, 1.3692, 1.3691, 1.3691, 1.3691, 1.3691, 1.3691, 1.3691, 1.388, 1.388, 1.388, 1.388, 1.388, 1.388, 1.3879, 1.3879, 1.3879, 1.3879, 1.3879, 1.3879, 1.3879, 1.3879, 1.3879, 1.3879, 1.3878, 1.3878, 1.3878, 1.3878, 1.3878, 1.3878, 1.3878, 1.3878, 1.3878, 1.3878, 1.3878, 1.3878, 1.3878, 1.3878, 1.6869, 1.6869, 1.6869, 1.6869, 1.6869, 1.6869, 1.6869, 1.6869, 1.6869, 1.6869, 1.6869, 1.6868, 1.6868, 1.6868, 1.6868, 1.6868, 1.6868, 1.6868, 1.6868, 1.6868, 1.6867, 1.6867, 1.6867, 1.6867, 1.6867, 1.6867, 1.6866, 1.6866, 1.6866, 1.6866, 1.7215, 1.7215, 1.7214, 1.7214, 1.7214, 1.7214, 1.7214, 1.7214, 1.7214, 1.7214, 1.7213, 1.7213, 1.7213, 1.7213, 1.7213, 1.7213, 1.7213, 1.7213, 1.7213, 1.7212, 1.7212, 1.7212, 1.7212, 1.7212, 1.7212, 1.7212, 1.7212, 1.7212, 1.7212, 1.7212, 2.0224, 2.0224, 2.0224, 2.0223, 2.0223, 2.0223, 2.0223, 2.0223, 2.0222, 2.0222, 2.0222, 2.0222, 2.0222, 2.0222, 2.0222, 2.0222, 2.0222, 2.0222, 2.0221, 2.0221, 2.0221, 2.0221, 2.0221, 2.0221, 2.022, 2.022, 2.022, 2.022, 2.022, 2.022]}, \"token.table\": {\"Topic\": [2, 4, 4, 3, 3, 3, 2, 3, 5, 3, 3, 1, 3, 3, 1, 4, 5, 3, 5, 1, 5, 4, 4, 3, 1, 1, 4, 1, 5, 5, 1, 5, 4, 2, 3, 1, 1, 5, 4, 1, 3, 4, 1, 2, 3, 2, 1, 2, 2, 2, 1, 2, 2, 2, 1, 5, 1, 2, 5, 4, 4, 4, 4, 2, 3, 1, 3, 2, 3, 3, 5, 2, 5, 1, 1, 4, 3, 5, 5, 3, 5, 5, 4, 5, 4, 1, 2, 5, 4, 2, 3, 5, 4, 3, 1, 1, 2, 1, 3, 5, 2, 2, 1, 2, 4, 5, 5, 2, 3, 2, 4, 1, 2, 4, 3, 5, 4, 2, 5, 3, 1, 5, 1, 2, 2, 4, 4, 1, 3, 4, 5, 4, 1, 2, 4, 1, 4, 3, 5, 2, 3, 4, 5, 1, 5, 5, 3, 3, 4, 2], \"Freq\": [0.9996704279221162, 0.9997249959624839, 0.999870386620685, 0.9998698378703278, 0.999833450695696, 0.9999368456276752, 0.9998030973971878, 0.9998124245117544, 0.9995289468047177, 0.9997880308903379, 0.999761767209703, 0.9999452861784045, 0.9997067341169545, 0.9997274321344386, 0.9997939727891672, 0.999700582891329, 0.9992081855623463, 0.9998504587244349, 0.9998535383340023, 0.9998372404560678, 0.9994797141774704, 0.9998395223399265, 0.9998604187686448, 0.9999269013081418, 0.9997405132399992, 0.9999483751451884, 0.9998898624168097, 0.9996649523454556, 0.9996859845566008, 0.99977027353075, 0.9998498743489269, 0.9998629920696297, 0.9994957534295364, 0.9999219850172422, 0.9998295081744817, 0.9999293087850843, 0.9995743028976816, 0.9995547008150394, 0.9998862273504958, 0.9998066678413846, 0.9998692041659454, 0.9998056558623329, 0.9996571606621032, 0.9998513378725569, 0.9999458792443265, 0.9996551871353732, 0.9996899967537887, 0.9999412446295336, 0.9997680192084364, 0.9998369802407209, 0.9998607551524472, 0.9997963313135619, 0.9997428160362316, 0.9998964246662301, 0.9997107307059944, 0.9996217924833438, 0.9999365253366557, 0.9998315790952459, 0.9996175018054382, 0.999554563695114, 0.9999473293527311, 0.9999513820434239, 0.9999580322018565, 0.9996896437261459, 0.9998772017574966, 0.9999159771715953, 0.9997679909938614, 0.9999302983954669, 0.9993150040217618, 0.9998035766412209, 0.9998883102676513, 0.9999101758706659, 0.9995961471597623, 0.9999154649659432, 0.9998258934527798, 0.9998992935997164, 0.9997025864667672, 0.9999175460084162, 0.9999313900375609, 0.9999124175306353, 0.9998675727839635, 0.9996527293908165, 0.9995371363902167, 0.99944866611812, 0.9998634867106124, 0.9995437941605787, 0.9998687881191566, 0.9998668654525896, 0.9999482316939711, 0.9998847173544358, 0.9999745023409786, 0.9998640735692411, 0.9994644181183827, 0.9997193088820313, 0.9998048837520905, 0.9998786778200426, 0.9999400511040846, 0.9997192414912168, 0.9997991463565472, 0.9997102790421593, 0.9999242780636552, 0.999755050889254, 0.999834332882904, 0.9996558711746896, 0.9998741574572451, 0.999867367378002, 0.9996014392872127, 0.9997260983856622, 0.9993016198846106, 0.9999172573680248, 0.9994555974106023, 0.9997377668204027, 0.9998398695674923, 0.9997877892305781, 0.9998099886967109, 0.9997976754262514, 0.9995949955781378, 0.9998658833628971, 0.9996145097888367, 0.9996531294636027, 0.9999053934233596, 0.9997562796221644, 0.9996522706114865, 0.9998562928793946, 0.9999262763903044, 0.9999189732139792, 0.9996379383943076, 0.9995045952833042, 0.9999753798143949, 0.9999314628287466, 0.9993875692702809, 0.9994163463128587, 0.9997976214718662, 0.999934433249003, 0.9995348558146986, 0.999711517677626, 0.9999576150967403, 0.9999735567138169, 0.9998714334933695, 0.999971830862948, 0.9998920151055928, 0.9998914479430769, 0.9994308932852337, 0.9998000219755477, 0.9994583020701148, 0.9999496568668823, 0.9998165704360963, 0.999846082987795, 0.9999074139411556, 0.9998090790083838], \"Term\": [\"...\", \"2012\", \"according\", \"also\", \"always\", \"american\", \"another\", \"around\", \"away\", \"back\", \"become\", \"best\", \"better\", \"big\", \"black\", \"call\", \"case\", \"change\", \"check\", \"child\", \"choice\", \"city\", \"come\", \"could\", \"couple\", \"day\", \"different\", \"divorce\", \"dress\", \"even\", \"ever\", \"every\", \"everyone\", \"experience\", \"family\", \"fashion\", \"favorite\", \"february\", \"feel\", \"find\", \"first\", \"food\", \"found\", \"friend\", \"get\", \"getting\", \"give\", \"go\", \"going\", \"good\", \"got\", \"great\", \"health\", \"heart\", \"help\", \"high\", \"home\", \"important\", \"issue\", \"keep\", \"kid\", \"know\", \"last\", \"let\", \"life\", \"like\", \"little\", \"live\", \"living\", \"long\", \"look\", \"looking\", \"lot\", \"love\", \"made\", \"make\", \"making\", \"many\", \"marriage\", \"may\", \"mean\", \"might\", \"mom\", \"moment\", \"month\", \"mother\", \"much\", \"must\", \"need\", \"never\", \"new\", \"new_york\", \"news\", \"next\", \"night\", \"often\", \"one\", \"parent\", \"part\", \"party\", \"people\", \"person\", \"photo\", \"place\", \"problem\", \"question\", \"read\", \"really\", \"recently\", \"relationship\", \"report\", \"right\", \"said\", \"say\", \"see\", \"show\", \"since\", \"something\", \"spring\", \"star\", \"still\", \"study\", \"style\", \"take\", \"thing\", \"think\", \"though\", \"three\", \"time\", \"today\", \"together\", \"turn\", \"two\", \"u\", \"use\", \"video\", \"want\", \"way\", \"wedding\", \"week\", \"well\", \"whether\", \"without\", \"woman\", \"word\", \"work\", \"world\", \"would\", \"year\", \"yet\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [2, 1, 5, 3, 4]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el20341122329850569314313592\", ldavis_el20341122329850569314313592_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el20341122329850569314313592\", ldavis_el20341122329850569314313592_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el20341122329850569314313592\", ldavis_el20341122329850569314313592_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=              x         y  topics  cluster       Freq\n",
       "topic                                                \n",
       "1      0.340172 -0.151894       1        1  25.425142\n",
       "0      0.031837  0.355130       2        1  24.955558\n",
       "4     -0.116283 -0.023638       3        1  18.507724\n",
       "2     -0.220675 -0.168728       4        1  17.879009\n",
       "3     -0.035050 -0.010869       5        1  13.232567, topic_info=          Term          Freq         Total Category  logprob  loglift\n",
       "144        one  15807.000000  15807.000000  Default  30.0000   30.000\n",
       "197       time  11992.000000  11992.000000  Default  29.0000   29.000\n",
       "996       look   8709.000000   8709.000000  Default  28.0000   28.000\n",
       "122       year   9596.000000   9596.000000  Default  27.0000   27.000\n",
       "10         new   8854.000000   8854.000000  Default  26.0000   26.000\n",
       "...        ...           ...           ...      ...      ...      ...\n",
       "953       high   1367.847107   1368.517584   Topic5  -5.3505    2.022\n",
       "475   february   1412.935639   1413.629488   Topic5  -5.3181    2.022\n",
       "351       case   1359.407670   1360.076929   Topic5  -5.3567    2.022\n",
       "2761     party   1331.716208   1332.386020   Topic5  -5.3773    2.022\n",
       "836     choice   1315.015166   1315.684532   Topic5  -5.3899    2.022\n",
       "\n",
       "[180 rows x 6 columns], token_table=      Topic      Freq       Term\n",
       "term                            \n",
       "3711      2  0.999670        ...\n",
       "7977      4  0.999725       2012\n",
       "17        4  0.999870  according\n",
       "771       3  0.999870       also\n",
       "2852      3  0.999833     always\n",
       "...     ...       ...        ...\n",
       "362       5  0.999950       work\n",
       "137       3  0.999817      world\n",
       "15        3  0.999846      would\n",
       "122       4  0.999907       year\n",
       "1478      2  0.999809        yet\n",
       "\n",
       "[150 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[2, 1, 5, 3, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "bigram_vis_5 = pyLDAvis.gensim.prepare(bigram_lda_model_5, corpus, id2word)\n",
    "bigram_vis_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de14162e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc7eb369",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Initialization\n",
    "D = len(texts)  # Number of documents\n",
    "V = len(id2word)  # Number of unique words\n",
    "T = 10  # Number of topics\n",
    "\n",
    "# Initialize topic assignment randomly\n",
    "topic_assignment = [[random.randint(0, T-1) for _ in document] for document in texts]\n",
    "\n",
    "# Initialize Count matrices\n",
    "# N_{d, t}: count of words in document d assigned to topic t\n",
    "ndt = np.zeros((D, T))  \n",
    "# N_{t, v}: count of assignments to topic t of word v\n",
    "ntv = np.zeros((T, V))\n",
    "# N_{t}: total count of words assigned to topic t\n",
    "nt = np.zeros(T)\n",
    "\n",
    "# Iterate over corpus to fill Count matrices\n",
    "for d in range(D):\n",
    "    for i, v in enumerate(texts[d]):\n",
    "        t = topic_assignment[d][i]\n",
    "        ntv[t, id2word.token2id[v]] += 1\n",
    "        ndt[d, t] += 1\n",
    "        nt[t] += 1\n",
    "\n",
    "# Iteratively update topic assignments\n",
    "for d in range(D):\n",
    "    for i, v in enumerate(texts[d]):\n",
    "        t = topic_assignment[d][i]\n",
    "        \n",
    "        # Decrement count matrices for old assignment\n",
    "        ntv[t, id2word.token2id[v]] -= 1\n",
    "        ndt[d, t] -= 1\n",
    "        nt[t] -= 1\n",
    "        \n",
    "        # Compute conditional distribution for new assignment\n",
    "        p = ((ndt[d, :] + 0.001) / (ndt[d, :].sum() + 0.001 * T)) * ((ntv[:, id2word.token2id[v]] + 0.001) / (nt.sum() + 0.001 * V))\n",
    "        assert np.all(p >= 0), \"Negative probabilities found!\"\n",
    "        assert np.all(p <= 1), \"Probabilities above 1 found!\"\n",
    "        assert not np.isnan(p).any(), \"Probabilities are NaN!\"\n",
    "        t = np.random.multinomial(1, p / p.sum()).argmax()\n",
    "        \n",
    "        # Increment count matrices for new assignment\n",
    "        ntv[t, id2word.token2id[v]] += 1\n",
    "        ndt[d, t] += 1\n",
    "        nt[t] += 1\n",
    "\n",
    "        # Update topic assignment\n",
    "        topic_assignment[d][i] = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be34bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10  # Number of words to display per topic\n",
    "\n",
    "for t in range(T):\n",
    "    print(f\"Topic {t}:\")\n",
    "    \n",
    "    # Get the top N word indices for this topic\n",
    "    top_word_indices = ntv[t].argsort()[::-1][:N]\n",
    "    \n",
    "    # Print the words\n",
    "    for i in top_word_indices:\n",
    "        print(f\"\\t{id2word[i]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfc3b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialization\n",
    "N = sum(len(doc) for doc in texts)  # Total number of words in all documents\n",
    "D = len(texts)  # Number of documents\n",
    "V = len(id2word)  # Number of unique words\n",
    "T = 10  # Number of topics\n",
    "alpha = 0.1  # Prior on the topic distributions per document\n",
    "\n",
    "# Initialize phi and gamma\n",
    "phi = np.ones((N, T)) / T\n",
    "gamma = np.full((T,), alpha + N / T)\n",
    "\n",
    "# Initialize term-topic matrix beta (you might use different method to initialize it)\n",
    "beta = np.random.dirichlet(alpha=np.ones(len(id2word)), size=T)\n",
    "\n",
    "# Map from word to its index\n",
    "word2id = {word: i for doc in texts for word in doc}\n",
    "\n",
    "\n",
    "# Expectation step\n",
    "def e_step():\n",
    "    global gamma  # Ensure we're using the global gamma variable\n",
    "    for n, doc in enumerate(texts):\n",
    "        for i, word in enumerate(doc):\n",
    "            for t in range(T):\n",
    "                phi[n][t] = beta[t][id2word.token2id[word]] * np.exp(psi(gamma[t]))\n",
    "            # Normalize phi\n",
    "            phi[n] /= phi[n].sum()\n",
    "        # Update gamma after processing each document\n",
    "        gamma = alpha + phi.sum(axis=0)\n",
    "\n",
    "# Maximization step\n",
    "def m_step():\n",
    "    for t in range(T):\n",
    "        for n, doc in enumerate(texts):\n",
    "            for i, word in enumerate(doc):\n",
    "                beta[t][word2id[word]] += phi[n][t]\n",
    "        # Normalize beta\n",
    "        beta[t] /= beta[t].sum()\n",
    "\n",
    "# Iterate until convergence\n",
    "max_iter = 100\n",
    "for iteration in range(max_iter):\n",
    "    e_step()\n",
    "    m_step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c4ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(beta, id2word, n_top_words):\n",
    "    for i, topic_dist in enumerate(beta):\n",
    "        topic_words = np.array(id2word)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "        print('Topic {}: {}'.format(i, ' '.join(topic_words)))\n",
    "\n",
    "# Call the function with your beta and id2word dictionary:\n",
    "n_top_words = 10\n",
    "display_topics(beta, list(id2word.values()), n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc53017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perplexity\n",
    "def compute_perplexity():\n",
    "    log_likelihood = 0\n",
    "    for n, doc in enumerate(texts):\n",
    "        for i, word in enumerate(doc):\n",
    "            log_likelihood += np.log(np.sum(phi[n, :] * beta[:, word2id[word]]))\n",
    "    perplexity = np.exp(-1. * log_likelihood / N)\n",
    "    return perplexity\n",
    "\n",
    "print(\"Perplexity: \", compute_perplexity())\n",
    "\n",
    "# Coherence (UMass measure)\n",
    "def compute_coherence():\n",
    "    topic_words = beta.argsort(axis=-1)[:, :10]  # Top 10 words per topic\n",
    "    total_score = 0\n",
    "    for t in range(T):\n",
    "        for i in range(len(topic_words[t]) - 1):\n",
    "            for j in range(i + 1, len(topic_words[t])):\n",
    "                score = np.log((beta[t, topic_words[t, j]] + 1.) / beta[t, topic_words[t, i]])\n",
    "                total_score += score\n",
    "    coherence = total_score / T\n",
    "    return coherence\n",
    "\n",
    "print(\"Coherence: \", compute_coherence())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43077c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Initialization\n",
    "D = len(texts)  # Number of documents\n",
    "V = len(id2word)  # Number of unique words\n",
    "T = 10  # Number of topics\n",
    "alpha = 0.1  # Prior on the topic distributions per document\n",
    "beta = 0.01  # Prior on the word distributions per topic\n",
    "\n",
    "# Initialize topic assignment randomly\n",
    "topic_assignment = [[random.randint(0, T-1) for _ in document] for document in texts]\n",
    "\n",
    "# Initialize Count matrices\n",
    "# N_{d, t}: count of words in document d assigned to topic t\n",
    "ndt = np.zeros((D, T))  \n",
    "# N_{t, v}: count of assignments to topic t of word v\n",
    "ntv = np.zeros((T, V))\n",
    "# N_{t}: total count of words assigned to topic t\n",
    "nt = np.zeros(T)\n",
    "\n",
    "# Iterate over corpus to fill Count matrices\n",
    "for d in range(D):\n",
    "    for i, v in enumerate(texts[d]):\n",
    "        t = topic_assignment[d][i]\n",
    "        ntv[t, id2word.token2id[v]] += 1\n",
    "        ndt[d, t] += 1\n",
    "        nt[t] += 1\n",
    "\n",
    "# Collapsed Gibbs Sampling\n",
    "for iteration in range(100):  # Choose the number of iterations\n",
    "    for d in range(D):\n",
    "        for i, v in enumerate(texts[d]):\n",
    "            t = topic_assignment[d][i]\n",
    "            \n",
    "            # Decrement count matrices for old assignment\n",
    "            ntv[t, id2word.token2id[v]] -= 1\n",
    "            ndt[d, t] -= 1\n",
    "            nt[t] -= 1\n",
    "            \n",
    "            # Sample new topic assignment from conditional distribution\n",
    "            p = ((ndt[d, :] + alpha) / (np.sum(ndt[d, :]) + T * alpha)) * ((ntv[:, id2word.token2id[v]] + beta) / (nt + V * beta))\n",
    "            t = np.random.multinomial(1, p / p.sum()).argmax()\n",
    "            \n",
    "            # Increment count matrices for new assignment\n",
    "            ntv[t, id2word.token2id[v]] += 1\n",
    "            ndt[d, t] += 1\n",
    "            nt[t] += 1\n",
    "            \n",
    "            # Update topic assignment\n",
    "            topic_assignment[d][i] = t\n",
    "\n",
    "# Get the word distributions for each topic\n",
    "phi = (ntv + beta) / (nt[:, None] + V * beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f892a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_lda_model_10 = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "\n",
    "bigram_lda_model_10.save(\"bigram_lda_model_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bfc886",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bigram_lda_model_10.print_topics())\n",
    "doc_lda = bigram_lda_model_10[corpus]\n",
    "\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', bigram_lda_model_10.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda_10 = CoherenceModel(model=bigram_lda_model_10, texts= cleaned_df_desc['lemmatized_short_description'], dictionary=id2word, coherence='c_v')\n",
    "coherence_lda_10 = coherence_model_lda_10.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5ac153",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "bigram_vis_10 = pyLDAvis.gensim.prepare(bigram_lda_model_10, corpus, id2word)\n",
    "bigram_vis_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb4b072",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCleaner:\n",
    "    def __init__(self, df, column_name, stop_words, wnl ):\n",
    "        self.df = df\n",
    "        self.column_name = column_name\n",
    "        self.stop_words = stop_words\n",
    "        self.wnl = wnl\n",
    "    \n",
    "    def remove_null(self, df, column_name):\n",
    "        df = df[df[column_name].notnull()]\n",
    "        return df\n",
    "\n",
    "    def remove_contractions(self, df, column_name):\n",
    "        df[f'RemoveContractions_{column_name}'] = df[column_name].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
    "        return df\n",
    "\n",
    "    def rebuild_string(self, df, column_name):\n",
    "        df[f'{column_name}_string_nocont'] = [' '.join(map(str, l)) for l in df[f'RemoveContractions_{column_name}']]\n",
    "        return df\n",
    "\n",
    "    def tokenize(self, df, column_name):\n",
    "        df[f'tokenized_{column_name}'] = df[f'{column_name}_string_nocont'].apply(word_tokenize)\n",
    "        return df\n",
    "    \n",
    "    def token_cleanup(self, df, column_name):\n",
    "        edge_cases = [\"``\", \"â€™\", \"''\", \"image\", \"title\", \"alt\", \"src\", \"width\", \"img\", \"http\", \"cbc\", \"jpg\", \"16x9_460\", \"buzzfeed\", \"com\", \"h1\", \"href\", \"href=\", 'p', '/p', '/a' \"rel\", \"www\", \"reuters\", \"timesofindia\", \"indiatimes\", \"margin\", \"nofollow\", '8217', '8230']\n",
    "        df[f'tokenized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [word.lower() for word in x])\n",
    "        df[f'tokenized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [word for word in x if word not in string.punctuation])\n",
    "        df[f'tokenized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [word for word in x if word not in self.stop_words])\n",
    "        df[f'tokenized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [word for word in x if '/' not in word])\n",
    "        df[f'tokenized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [word for word in x if word not in edge_cases])\n",
    "        return df\n",
    "\n",
    "    def make_bigrams(self, df, column_name):\n",
    "        bigram = gensim.models.Phrases(df[f'tokenized_{column_name}'], min_count=5, threshold=100)\n",
    "        bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "        def get_bigrams(tokens_list):\n",
    "            return bigram_mod[tokens_list]\n",
    "\n",
    "        df[f'bigrams_{column_name}'] = df[f'tokenized_{column_name}'].apply(get_bigrams)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    def lemmatize_tokens(self, df, column_name):\n",
    "        clean_up = [\"'s\", \"--\"]\n",
    "        df[f'lemmatized_{column_name}'] = df[f'tokenized_{column_name}'].apply(lambda x: [self.wnl.lemmatize(word) for word in x])\n",
    "        df[f'lemmatized_{column_name}'] = df[f'lemmatized_{column_name}'].apply(lambda x: [word for word in x if word not in clean_up])\n",
    "        return df\n",
    "\n",
    "\n",
    "    def clean(self):\n",
    "        df = self.remove_null(self.df, self.column_name)\n",
    "        df = self.remove_contractions(df, self.column_name)\n",
    "        df = self.rebuild_string(df, self.column_name)\n",
    "        df = self.tokenize(df, self.column_name)\n",
    "        df = self.token_cleanup(df, self.column_name)\n",
    "       # df = self.make_bigrams(df, self.column_name)\n",
    "        df = self.lemmatize_tokens(df, self.column_name)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3e87b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('News_Category_Dataset_v3.json', lines=True)\n",
    "wnl = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "desc_cleaner_news_uni = DataCleaner(df, 'short_description', stop_words, wnl)\n",
    "cleaned_df_desc_uni = desc_cleaner_news_uni.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea59c756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word_uni = corpora.Dictionary(cleaned_df_desc_uni['lemmatized_short_description'])\n",
    "\n",
    "# Create Corpus\n",
    "texts_uni = cleaned_df_desc_uni['lemmatized_short_description']\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus_uni = [id2word_uni.doc2bow(text) for text in texts_uni]\n",
    "\n",
    "# View the first entry in the corpus\n",
    "print(corpus_uni[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f2c252",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_uni = gensim.models.ldamodel.LdaModel(corpus=corpus_uni,\n",
    "                                           id2word=id2word_uni,\n",
    "                                           num_topics=5, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9473a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model_uni.save(\"unigram_lda_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db5317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_model_uni.print_topics())\n",
    "doc_lda_uni = lda_model_uni[corpus_uni]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671ff682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model_uni.log_perplexity(corpus_uni))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda_uni = CoherenceModel(model=lda_model_uni, texts= cleaned_df_desc['lemmatized_short_description'], dictionary=id2word_uni, coherence='c_v')\n",
    "coherence_lda_uni = coherence_model_lda_uni.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda_uni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e719a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model_uni, corpus_uni, id2word_uni)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aaf685",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb01dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.save(\"bigram_lda_model_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fa570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8892964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts= cleaned_df_desc['lemmatized_short_description'], dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4e167a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "probstoch_conda",
   "language": "python",
   "name": "probstoch_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
